---
title: Representing Pitch
output:
    learnr::tutorial:
      theme: flatly
      progressive: true
      allow_skip: true
      ace_theme: "github"
      df_print: "kable"
      css: css/mycss.css
runtime: shiny_prerendered
description: >
  Discuss different ways pitch can be represented for research purposes, and illustrate relevant 
  functionality in humdrumR.
---

```{r, include = FALSE, message =FALSE, warning=FALSE}
library(learnr)
library(humdrumR)
humdrumR(syntaxHighlight = FALSE, maxRecordsPerFile = 30L)
source('helper_functions.R')
tutorial_options(exercise.lines = 10)

```

<script language="JavaScript" src="js/scripts.js"></script>

## What is Pitch

Pitch is a subjective perceptual phenomenon.

- Humans *perceive* some sounds as having a property called "pitch."
  - The percept is roughly associated with the fundamental frequency of harmonic signals.
- However, we mainly perceive relationships between pitches, which are the basis for musical organization.
  + Melodic contours
  + Tonal hierarchies


### Data

Let's get some data: 

+ The trusty 371 Bach Chorales^[Actually 370, because I'll leave out the one five-voice chorale.].
+ Vocal melodies from the Coordinated Corpus of Popular music.

```{r,  message = FALSE}

# bach <- readHumdrum('../Data/Bach370/.*krn')
load('../Data/Bach370/Bach.Rd')

# cocopops <- readHumdrum('../Data/CoCoPops/.*hum', reference = c('BillboardChartDate', 'BillboardPeak', 'COC', 'OTL'))
load('../Data/CoCoPops/CoCoPops.Rd')

```

```{r}
bach[1]
```





## Representing Pitch


Information about pitch in music can be represented in numerous ways.

###

Humdrum$_{\mathcal{R}}$ can read/write most common representations.

```{r, echo = FALSE}

pline()

lab(.95, '**deg **degree **solfa **bhatk')
lab(.75, '**kern **pitch')
lab(.3, '**semits **midi')
lab(.05, '**freq')


```

###

```{r, echo = FALSE}

pitchRepTable <-  data.frame(Interpretation = c('**deg', '**degree','**solfa', '**bhatk', '**kern', '**pitch', '**semits', '**midi',  '**freq'), 
                             Concept        = c('**Scale degree**', '', '', '', '**Absolute pitch**',  '', '', '', '**Frequency**'),
                             Type           = c('',             '', '', '',  '',    '$\\uparrow$ More tonal', '$\\downarrow$More acoustic', '','') )
knitr::kable(pitchRepTable, 'pipe')


```


## Conversion

We can (and frequently do ) convert between different representations of pitch.

### Lossless

Some pitch representations convey exactly the same information---the only difference is readability and aesthetics.

We can convert between equivalent representations without losing and information (**lossless**).



```{r lossless, exercise = TRUE}
Kern  <- c('c', 'g#','a', 'dd','cc')
Pitch <- c('C4', 'G#4', 'A4', 'D5', 'C5')
```

```{r lossless-solution}
data.frame(Pitch2Kern = Pitch |> kern(),
           Kern2Pitch = Kern |> pitch(),
           Pitch2Kern2Pitch = Pitch |> kern() |> pitch())

```

```{r lossless-hint}
Kern |> tonh() |> kern()

```


### Lossy

In contrast, some pitch representation really do represent different information about pitch.

Converting between these representations can lose information (**lossy**), or conversely *require* additional information (depending on the direction you are converting).

```{r lossy, exercise = TRUE}
Kern  <- c('c', 'a-','a', 'dd','cc')
Freq <- c(261.6256, 415.3047, 440.0000, 587.3295, 523.2511)

```

```{r lossy-solution}

data.frame(Freq2Kern = Freq |> kern(Exclusive = 'freq'),
           Kern2Freq = Kern |> freq(frequency.reference = 440, frequency.reference.note = 'a', tonalHarmonic = 2^(19/12)),
           Kern2Freq2Kern = Kern |> freq() |> kern())


```

<div id='lossy-hint'>
The distinction between G# and Ab is lost in the frequency representation.
Conversely, information about exact pitch is lost in the kern representation.

We have to independently provide additional information about the tuning system in order to convert between these representations at all.
</div>



###

When doing computational musicology research, we need to consider: 

+ What information about pitch is encoded in our data?
+ If we are creating our own data, what information should we put into our data?





## Analyzing Pitch

What kind of analyses can we support with this data?


```{r fig.height=10, fig.width=6}
bach |> freq() |> with(draw( . ,facet = Instrument, showCounts = FALSE))

```


However, this data doesn't really support this analysis.

+ The exact tuning system used by Bach, or any particular choir singing this music, is not represented in the data.
+ Nor is the tuning variations that would happen in real human performance.

###


We might get closer with A420 and a Pythagorean tuning:

```{r fig.height=10, fig.width=6}
bach |> freq() |> with(draw(. ,facet = Instrument, showCounts = FALSE, xlim = c(20, 500), breaks = 30))

bach |> freq(tonalHarmonic = 3, frequency.reference = 420) |> with(draw(. ,facet = Instrument, showCounts = FALSE, xlim = c(20, 500), breaks = 30))

```

```{r}

pline()

lab(.85, '**kern **pitch')
lab(.05, '**freq')


```

### Using frequency

What research questions might be supported by frequency data?

<div id='freq-hint'>
+ Estimating reverb tail of choir singing in specific concert hall.
</div>

----

### Tonality


Frequency space is not very representative of musical experience.
We can try something more perceptually meaningful using semitones:


```{r fig.height=10, fig.width=6}
bach |> semits() |> with(draw(Semits ,facet = Instrument, showCounts = FALSE, xlim = c(20, 500), breaks = 30))

```

This representation can be fully extracted from our score data.





## Mathematical Assumptions


