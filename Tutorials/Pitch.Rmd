---
title: Representing Pitch
output:
    learnr::tutorial:
      theme: sandstone
      progressive: true
      allow_skip: true
      ace_theme: "github"
      df_print: "kable"
      css: css/mycss.css
runtime: shiny_prerendered
description: >
  Discuss different ways pitch can be represented for research purposes, and illustrate relevant 
  functionality in humdrumR.
---

```{r, include = FALSE, message =FALSE, warning=FALSE}
library(learnr)
library(humdrumR)
humdrumR(syntaxHighlight = FALSE, maxRecordsPerFile = 30L)
source('helper_functions.R')
tutorial_options(exercise.lines = 10)

```

<script language="JavaScript" src="js/scripts.js"></script>

## Representing Pitch

<!-- Pitch is a complex, subjective perceptual phenomenon. -->

<!-- Humans *perceive* some sounds as having a property called "pitch." -->
<!-- The percept is roughly associated with the fundamental frequency of harmonic signals. -->
<!-- However, we mainly perceive relationships between pitches, which are the basis for musical organization. -->
<!-- Melodic contours; -->
<!-- Tonal hierarchies; etc. -->

<!-- ## -->

In Western music theory and practice, we have a number of ways of representing the "pitch" of note events.

+ Frequency
+ Cents
+ Semitones
+ Pitch classes
+ Steps (generic)
+ Tonal name (specific)
+ Scale degrees
+ Intervals



### Dimensions {.tabset}

These represent different aspects/dimensions of the construct "pitch."

#### Acoustic vs Perceptual

```{r, echo = FALSE, out.width = 600, out.height=300, fig.height=2.5, fig.width = 5}
pitch_space1("Acoustic", "Perceptual")

lab(0, 'Frequency')
lab(0.2, "Cents")
lab(.6, 'Semitones')
lab(1, "Scale degrees")
lab(.8, "Tonal interval")

```

#### Atonal vs Tonal

```{r, echo = FALSE, out.width = 600, out.height=300, fig.height=2.5, fig.width = 5}
pitch_space1("Atonal", "Tonal")

lab(0, "Frequency")
lab(.2, 'Semitones')
lab(.8 , 'Scientific Pitch')
lab(1, "Scale degree")

```

#### Absolute vs Relative


```{r, echo = FALSE, out.width = 600, out.height=300, fig.height=2.5, fig.width = 5}
pitch_space1("Absolute", "Relative")

lab(0, 'Frequency')
lab(0.2, 'Semitones')
lab(0.3, 'Scientific pitch')
lab(.8, 'Scale degree')

```

### Humdrum {.tabset}

#### Construct

Humdrum representations for all of these things are defined:

```{r, echo = FALSE, out.width = 600, out.height=600, fig.height=5, fig.width = 5}

pitch_space2('Atonal', 'Tonal', 'Absolute', 'Relative')

lab2(0, 0, 'Frequency')
lab2(0.2, 0.2, 'Semitones')
lab2(.8, .3, 'Scientific pitch')
lab2(1, .8, 'Scale degree')

```

#### Humdrum

Humdrum representations for all of these things are defined:

```{r, echo = FALSE, out.width = 600, out.height=600, fig.height=5, fig.width = 5}

pitch_space2('Atonal', 'Tonal', 'Absolute', 'Relative')

lab2(0, 0, '**freq')
lab2(0.2, 0.2, '**semits')
lab2(.8, .3, '**pitch\n**kern')
lab2(1, .8, '**deg\n**solfa')

```

#### HumdrumR

Humdrum$_{\mathcal{R}}$ can read/write most common representations.

```{r, echo = FALSE, out.width = 600, out.height=600, fig.height=5, fig.width = 5}


pitch_space2('Atonal', 'Tonal', 'Absolute', 'Relative')

lab2(0, 0, 'freq()', family = 'mono')
lab2(0.2, 0.2, 'semits()', family = 'mono')
lab2(.8, .3, 'pitch()\nkern()', family = 'mono')
lab2(1, .8, 'deg()\nsolfa()', family = 'mono')


```

###

Some information is encoded directly in scores/data.

Other information must be inferred/understood (*domain knowledge*) from data.

This means that translation can be lossy/lossless...

----



## Data

Let's get some data: 

+ The trusty 371 Bach Chorales^[Actually 370, because I'll leave out the one five-voice chorale.].
+ Vocal melodies from the [Coordinated Corpus of Popular music](https://github.com/Computational-Cognitive-Musicology-Lab/CoCoPops).
+ F0 contours from [Saraga](https://mtg.github.io/saraga/) (Carnatic music)

```{r,  message = FALSE, eval = FALSE}

bach <- readHumdrum('Data/Bach370/.*krn')
cocopops <- readHumdrum('Data/CoCoPops/.*hum', reference = c('BillboardChartDate', 'BillboardPeak', 'COC', 'OTL'))
saraga <- readHumdrum('Data/Saraga/.*freq')

load('../Data/Bach370/Bach.Rd')

```

```{r}
load('../Data/CoCoPops/CoCoPops.Rd')

```


###

These datasets represent radically different sources of pitch information:

+ *Bach Chorales*: Explicitly notated scores. Pre-EQT/440.
+ *CoCoPops*: By-ear transcriptions of recordings. Modern EQT/440 era, but includes blue-inflected music.
+ *Saraga*: Machine extraction of continuous [$F_0$](https://en.wikipedia.org/wiki/Fundamental_frequency) contours.

###

```{r data, exercise = TRUE, fig.cap = "We can inspect files from each dataset."}

bach[1]

census(bach)
# cocopops[13]

# saraga[2]

```


## Entropy

[Entropy](https://en.wikipedia.org/wiki/Entropy_(information_theory)), also called "Shannon entropy"^[After Claude Shannon.] or "information entropy," is a useful way of characterizing how certain/uncertain *any* probability distribution is.
Entropy is the central concept if [information theory](https://en.wikipedia.org/wiki/Information_theory).

###

Entropy is essentially the average (log) probability of observations; 

$$
H(X) := -\sum(P(X)\log_2(P(X))
$$

$$
H(X) := mean(-{\log_2 P(X)})
$$


High probability events are uninformative, predictable.
Low probability events are informative, unpredictable, surprising.

If a distribution is totally predictable the entropy is 0.
The more a distribution is made up of low-probability events, the higher the entropy.

This effectively means that entropy is a measure of how "flat" a distribution is.
Compare these two distributions:

| Rock | Reggae | Blues | Pop | Country | Jazz | Film | Classical | Metal | Punk | Funk | R&B              | Folk |
|------|--------|-------|-----|---------|------|------|-----------|-------|------|------|------------------|------|
| .14  | .04    | .09   | .15 | .13     | .08  | .03  | .03       | .04   | .07  | .05  | .08              | .06  |
| .12  | .02    | .02   | .67 | .06     | .01  | .01  | .01       | .01   | .01  | .01  | .01              | .02  |

```{r entropy_genre, echo = FALSE}

par(family = 'Times', mar = c(5,4,2,2), mfcol = c(1,2))
row1 <- c(.14,.04,.09,.15,.13,.08,.03,.03,.04,.07,.05,.08,.06)
row2 <- c(.12,.02,.02,.67,.06,.01,.01,.01,.01,.01,.01,.01,.02)
genres <- c("Rock","Reggae","Blues","Pop","Country","Jazz","Film","Classical","Metal","Punk","Funk","R&B","Folk")
x <- barplot(row1, main = 'Genre Distribution (Row 1)', axes =FALSE, ylim = c(0,1), space = 0)
axis(2, pretty(c(0, 1)), tick =FALSE, las = 1)
axis(1, x, genres, las = 2, tick = FALSE)

text(mean(x), .8, paste0('Entropy (bits) = ', round(-sum(row1 * log(row1, 2)), 2)))


x <- barplot(row2, main = 'Genre Distribution (Row 2)', axes =FALSE, ylim = c(0,1), space = 0)
axis(2, pretty(c(0, 1)), tick =FALSE, las = 1)
axis(1, x, genres, las = 2, tick = FALSE)

text(mean(x), .8, paste0('Entropy (bits) = ', round(-sum(row2 * log(row2, 2)), 2)))

```

The second row has a lower entropy because it is much more concentrated on a few values (especially Pop).
When we observe samples from this distribution, more than half the time we observe Pop, so we aren't surprised much.
We do occasionally get surprised by a rare draw of Folk or Blues, but only very rarely---most of the time, we see the commonplace Pop.
In contrast, when we draw from the row-one distribution, every observation is a bit if a surprise, so the entropy is higher.

The key thing is that entropy can be computed even when distributions differ in the number of levels, or even if there are infinite levels (continous variable).
For example, the following distribution, with only four categories, also has 1.8 bits of entropy:

| Rock | Reggae | Blues | Pop |
|------|--------|-------|-----|
| .46  | .24    | .2    | .1  |


As so does this distribution:

```{r, echo = FALSE}
par(mar= c(2,2,2,2), mfcol=c(1,1))
curve(dnorm(x, 0, 1.191604), -5, 5, axes = FALSE)
mtext('Value', 1, line = 3)
mtext('Density', 2, line = 3)
axis(1, seq(-5,5,.5), las = 1, tick = FALSE,
     cex.axis = .5)
axis(2, seq(0,.3, .05), tick = FALSE, las = 1)

```



### Additive property

A cool property of entropy is that, if we combine distributions that are independent, their *joint entropy* is the sum of their independent entropies.
For example, the entropy of a coin flip is one bit; the entropy of two independent coin flips is two bits. Etc.

However, if two distributions are dependent, then their joint entropy will be less than the sum of the independent entropies.
This difference is called the mutual information.

###

How much mutual information is there between pitch and rhythm information?

```{r mutual, exercise = TRUE}


```

```{r mutual-solution}
# It depends on whether we use absolute or relative pitch.

```






## Analyses

### 

```{r mostFrequentPitch, exercise = TRUE, fig.cap = 'What is the most frequent pitch in each dataset?'}


```

### Prediction


### Distance


How can we represent "distance" in pitch?



### Shape

How can we express "shape" in pitch.

### Repetition

How can we express "repetition" in pitch?
